{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AGE AND GENDER DETECTION IN A MEETING ROOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the UTKFace dataset for age and gender detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the UTKFace dataset\n",
    "UTK = \"UTKFace\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "data = []\n",
    "gender_labels = []\n",
    "age_labels = []\n",
    "\n",
    "for img_name in os.listdir(UTK):\n",
    "    img_path = os.path.join(UTK, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (64, 64))\n",
    "    \n",
    "    age_label = int(img_name.split(\"_\")[0])  # Age label\n",
    "    gender_label = int(img_name.split(\"_\")[1])  # Gender label\n",
    "    \n",
    "    data.append(img)\n",
    "    gender_labels.append(gender_label)\n",
    "    age_labels.append(age_label)\n",
    "\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "gender_labels = np.array(gender_labels)\n",
    "age_labels = np.array(age_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "(trainX, testX, trainGenderY, testGenderY) = train_test_split(data, gender_labels, test_size=0.2, random_state=42)\n",
    "(trainX, testX, trainAgeY, testAgeY) = train_test_split(data, age_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the gender classification model\n",
    "gender_model = Sequential()\n",
    "gender_model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", input_shape=(64, 64, 3)))\n",
    "gender_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "gender_model.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "gender_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "gender_model.add(Flatten())\n",
    "gender_model.add(Dense(128, activation=\"relu\"))\n",
    "gender_model.add(Dropout(0.5))\n",
    "gender_model.add(Dense(1, activation=\"sigmoid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the gender model\n",
    "gender_model.compile(loss=\"binary_crossentropy\", optimizer=Adam(0.001), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.6984 - loss: 0.5734 - val_accuracy: 0.8596 - val_loss: 0.3176\n",
      "Epoch 2/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 49ms/step - accuracy: 0.8561 - loss: 0.3253 - val_accuracy: 0.8809 - val_loss: 0.2684\n",
      "Epoch 3/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 47ms/step - accuracy: 0.8811 - loss: 0.2789 - val_accuracy: 0.8874 - val_loss: 0.2589\n",
      "Epoch 4/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 47ms/step - accuracy: 0.8898 - loss: 0.2649 - val_accuracy: 0.8846 - val_loss: 0.2702\n",
      "Epoch 5/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 47ms/step - accuracy: 0.8965 - loss: 0.2403 - val_accuracy: 0.8943 - val_loss: 0.2443\n",
      "Epoch 6/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 48ms/step - accuracy: 0.9117 - loss: 0.2173 - val_accuracy: 0.8975 - val_loss: 0.2407\n",
      "Epoch 7/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 48ms/step - accuracy: 0.9026 - loss: 0.2208 - val_accuracy: 0.8984 - val_loss: 0.2511\n",
      "Epoch 8/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 49ms/step - accuracy: 0.9166 - loss: 0.1956 - val_accuracy: 0.8996 - val_loss: 0.2507\n",
      "Epoch 9/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 49ms/step - accuracy: 0.9209 - loss: 0.1861 - val_accuracy: 0.8975 - val_loss: 0.2407\n",
      "Epoch 10/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 50ms/step - accuracy: 0.9296 - loss: 0.1725 - val_accuracy: 0.9040 - val_loss: 0.2432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2bba15b75d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the gender model\n",
    "gender_model.fit(trainX, trainGenderY, validation_data=(testX, testGenderY), batch_size=32, epochs=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the gender model\n",
    "gender_model.save(\"gender_classification_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the age detection model\n",
    "age_model = Sequential()\n",
    "age_model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", input_shape=(64, 64, 3)))\n",
    "age_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "age_model.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "age_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "age_model.add(Flatten())\n",
    "age_model.add(Dense(128, activation=\"relu\"))\n",
    "age_model.add(Dropout(0.5))\n",
    "age_model.add(Dense(1, activation=\"linear\"))  # Linear activation for age regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the age model\n",
    "age_model.compile(loss=\"mean_squared_error\", optimizer=Adam(0.001), metrics=[\"mae\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1965s\u001b[0m 3s/step - loss: 420.6267 - mae: 15.5906 - val_loss: 183.2208 - val_mae: 10.1478\n",
      "Epoch 2/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 52ms/step - loss: 210.7736 - mae: 10.9990 - val_loss: 154.7344 - val_mae: 9.2144\n",
      "Epoch 3/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 47ms/step - loss: 176.8996 - mae: 10.0405 - val_loss: 155.8442 - val_mae: 9.1225\n",
      "Epoch 4/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 47ms/step - loss: 165.2999 - mae: 9.6416 - val_loss: 120.0314 - val_mae: 8.0316\n",
      "Epoch 5/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 47ms/step - loss: 147.7255 - mae: 9.0786 - val_loss: 118.4013 - val_mae: 7.8741\n",
      "Epoch 6/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 48ms/step - loss: 134.5982 - mae: 8.6564 - val_loss: 117.4112 - val_mae: 7.8542\n",
      "Epoch 7/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 48ms/step - loss: 128.9658 - mae: 8.3994 - val_loss: 140.2838 - val_mae: 8.5267\n",
      "Epoch 8/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 50ms/step - loss: 126.7289 - mae: 8.3848 - val_loss: 122.7733 - val_mae: 7.8921\n",
      "Epoch 9/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 49ms/step - loss: 117.2634 - mae: 8.1060 - val_loss: 110.8926 - val_mae: 7.5149\n",
      "Epoch 10/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 49ms/step - loss: 116.5909 - mae: 8.0238 - val_loss: 104.0102 - val_mae: 7.3454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2bba6c975d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the age model\n",
    "age_model.fit(trainX, trainAgeY, validation_data=(testX, testAgeY), batch_size=32, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the age model\n",
    "age_model.save(\"age_detection_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to images and masks\n",
    "image_dir = \"C:/Users/SARATHLAL/Downloads/null class projects/3.meeting_room_det/DeepFashion_In-shop_Clothes_Retrieval_Adjusted/images\"\n",
    "mask_dir = \"C:/Users/SARATHLAL/Downloads/null class projects/3.meeting_room_det/DeepFashion_In-shop_Clothes_Retrieval_Adjusted/masks\"\n",
    "output_dir = \"output_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories if they don't exist\n",
    "os.makedirs(os.path.join(output_dir, \"white\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"black\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"other\"), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_shirt(image, mask):\n",
    "    # Apply the mask to the image\n",
    "    segmented = cv2.bitwise_and(image, image, mask=mask)\n",
    "    return segmented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_shirt_color(segmented_shirt):\n",
    "    # Convert segmented shirt to HSV\n",
    "    hsv_img = cv2.cvtColor(segmented_shirt, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Define color ranges for white and black\n",
    "    mask_white = cv2.inRange(hsv_img, (0, 0, 200), (180, 20, 255))\n",
    "    mask_black = cv2.inRange(hsv_img, (0, 0, 0), (180, 255, 30))\n",
    "    \n",
    "    white_pixels = cv2.countNonZero(mask_white)\n",
    "    black_pixels = cv2.countNonZero(mask_black)\n",
    "    \n",
    "    if white_pixels > black_pixels:\n",
    "        return \"white\"\n",
    "    elif black_pixels > white_pixels:\n",
    "        return \"black\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "# Process each image and mask\n",
    "for img_name in os.listdir(image_dir):\n",
    "    img_path = os.path.join(image_dir, img_name)\n",
    "    mask_path = os.path.join(mask_dir, img_name)\n",
    "    \n",
    "    image = cv2.imread(img_path)\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if image is not None and mask is not None:\n",
    "        # Segment the shirt region\n",
    "        segmented_shirt = segment_shirt(image, mask)\n",
    "        \n",
    "        # Detect the shirt color\n",
    "        shirt_color = detect_shirt_color(segmented_shirt)\n",
    "        \n",
    "        # Save the segmented shirt image in the appropriate directory\n",
    "        output_path = os.path.join(output_dir, shirt_color, img_name)\n",
    "        cv2.imwrite(output_path, segmented_shirt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train the Shirt Color Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to dataset\n",
    "dataset_path = \"C:/Users/SARATHLAL/Downloads/null class projects/3.meeting_room_det/DeepFashion_In-shop_Clothes_Retrieval_Adjusted/images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "img_height, img_width = 64, 64\n",
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of classes\n",
    "num_classes = len(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')  # Update to use num_classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SARATHLAL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 253ms/step - accuracy: 0.9811 - loss: 0.0315 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177us/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 202ms/step - accuracy: 1.0000 - loss: 4.3101e-07 - val_accuracy: 1.0000 - val_loss: 1.9436e-10\n",
      "Epoch 4/10\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143us/step - accuracy: 1.0000 - loss: 3.7253e-09 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 201ms/step - accuracy: 1.0000 - loss: 1.5774e-08 - val_accuracy: 1.0000 - val_loss: 6.4788e-11\n",
      "Epoch 6/10\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108us/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 202ms/step - accuracy: 1.0000 - loss: 1.3485e-08 - val_accuracy: 1.0000 - val_loss: 2.9154e-10\n",
      "Epoch 8/10\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102us/step - accuracy: 1.0000 - loss: 1.4901e-08 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 205ms/step - accuracy: 1.0000 - loss: 1.8764e-08 - val_accuracy: 1.0000 - val_loss: 3.2394e-11\n",
      "Epoch 10/10\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112us/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d46c7969d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save(\"shirt_color_classification_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths to the model files\n",
    "weights_path = \"DeepFashion_In-shop_Clothes_Retrieval_Adjusted/yolov3.weights\"  # Path to yolov3 weights file\n",
    "config_path = \"DeepFashion_In-shop_Clothes_Retrieval_Adjusted/yolov3.cfg\"       # Path to yolov3 config file\n",
    "labels_path = \"DeepFashion_In-shop_Clothes_Retrieval_Adjusted/coco.names\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
     ]
    }
   ],
   "source": [
    "# Load the COCO class labels\n",
    "with open(labels_path, 'r') as f:\n",
    "    labels = f.read().strip().split(\"\\n\")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv3 model\n",
    "net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# Load the pre-trained gender classification model\n",
    "gender_model = load_model(\"gender_classification_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained shirt color classification model\n",
    "shirt_color_model = load_model(\"shirt_color_classification_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image path\n",
    "image_path = \"C:/Users/SARATHLAL/Downloads/null class projects/DeepFashion_In-shop_Clothes_Retrieval_Adjusted/meeting room.jpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the file exists\n",
    "if not os.path.isfile(image_path):\n",
    "    print(f\"Error: File '{image_path}' not found.\")\n",
    "else:\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    if image is None:\n",
    "        print(f\"Error: Failed to load image '{image_path}'.\")\n",
    "    else:\n",
    "        # Get image dimensions\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        # Prepare the image for YOLO\n",
    "        blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), swapRB=True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward(output_layers)\n",
    "\n",
    "        # Initialize lists to hold the detection results\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        class_ids = []\n",
    "\n",
    "        # Process each detection\n",
    "        for output in detections:\n",
    "            for detection in output:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if labels[class_id] == \"person\" and confidence > 0.5:\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        # Apply non-maxima suppression to filter the detections\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "        # Only proceed if there are at least 2 people detected\n",
    "        if len(indices) >= 2:\n",
    "            for i in indices.flatten():\n",
    "                x, y, w, h = boxes[i]\n",
    "                person_img = image[y:y+h, x:x+w]\n",
    "\n",
    "                # Resize the person image for gender classification\n",
    "                person_img_resized = cv2.resize(person_img, (64, 64))\n",
    "                person_img_resized = np.expand_dims(person_img_resized, axis=0) / 255.0\n",
    "\n",
    "                # Gender classification\n",
    "                gender_pred = gender_model.predict(person_img_resized)\n",
    "                gender = 'Male' if gender_pred[0][0] > 0.5 else 'Female'\n",
    "\n",
    "                # Resize the person image for shirt color classification\n",
    "                person_img_resized = cv2.resize(person_img, (64, 64))\n",
    "                person_img_resized = np.expand_dims(person_img_resized, axis=0) / 255.0\n",
    "\n",
    "                # Shirt color classification\n",
    "                shirt_color_pred = shirt_color_model.predict(person_img_resized)\n",
    "                shirt_color = np.argmax(shirt_color_pred, axis=1)[0]\n",
    "\n",
    "                if shirt_color == 0:  # Assuming 0 is white\n",
    "                    age = 23\n",
    "                    shirt_color_str = \"White\"\n",
    "                elif shirt_color == 1:  # Assuming 1 is black\n",
    "                    age = \"Child\"\n",
    "                    shirt_color_str = \"Black\"\n",
    "                else:\n",
    "                    age = \"Unknown\"\n",
    "                    shirt_color_str = \"Other\"\n",
    "\n",
    "                color = (0, 255, 0)  # Green color for bounding box\n",
    "                label = f\"{gender}, Age: {age}, Shirt: {shirt_color_str}\"\n",
    "                cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        # Display the output image\n",
    "        cv2.imshow(\"Detected People\", image)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
